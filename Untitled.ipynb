{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]),\n",
       " [tensor([0., 1., 2.], grad_fn=<SliceBackward>),\n",
       "  tensor([3., 4., 5.], grad_fn=<SliceBackward>),\n",
       "  tensor([6., 7., 8.], grad_fn=<SliceBackward>)])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def stable_arg_sort_long(arr):\n",
    "    \"\"\"Stable sort of long tensors.\n",
    "\n",
    "    Note that Pytorch 1.5.0 does not have a stable sort implementation.\n",
    "    Here we simply add a delta value between 0 and 1 (exclusive) and\n",
    "    assuming we are using integers, call\n",
    "    torch.argsort to get a stable sort.\"\"\"\n",
    "    delta = torch.linspace(0, 0.99, arr.shape[0])\n",
    "    return torch.argsort(arr + delta)\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "def unique_with_counts(arr: torch.Tensor, grouped: Dict[int, int]):\n",
    "    \"\"\"\n",
    "    Equivalent to `np.unqiue(x, return_counts=True)`\n",
    "    \n",
    "    :param arr:\n",
    "    :param grouped:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for x in arr:\n",
    "        if x.item() not in grouped:\n",
    "            grouped[x.item()] = 1\n",
    "        else:\n",
    "            grouped[x.item()] += 1\n",
    "\n",
    "    counts = torch.zeros(len(grouped), dtype=torch.long)\n",
    "    values = torch.empty(len(grouped), dtype=arr.dtype)\n",
    "    for i, (k, v) in enumerate(grouped.items()):\n",
    "        values[i] = k\n",
    "        counts[i] = v\n",
    "    a = torch.argsort(values)\n",
    "\n",
    "    return values[a], counts[a]\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _jit_scatter_group(x: torch.Tensor, idx: torch.Tensor, d: Dict[int, int]) -> Tuple[\n",
    "    torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Assume idx is a sorted index\n",
    "\n",
    "    :param x:\n",
    "    :param idx:\n",
    "    :param d:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    arg = stable_arg_sort_long(idx)\n",
    "    x = x[arg]\n",
    "    groups, b = unique_with_counts(idx, d)\n",
    "    i_a = 0\n",
    "    arr_list = []\n",
    "    for i_b in b:\n",
    "        arr_list.append(x[i_a:i_a + i_b.item()])\n",
    "        i_a += i_b.item()\n",
    "    return groups, arr_list\n",
    "\n",
    "\n",
    "def scatter_group(x: torch.Tensor, idx: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Group a tensor by indices. This is equivalent to successive applications of `x[torch.where(x == index)]`\n",
    "    for all provided sorted indices\n",
    "\n",
    "    Example:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        idx = torch.tensor([2, 2, 0, 1, 1, 1, 2])\n",
    "        x = torch.tensor([0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "        uniq_sorted_idx, out = scatter_group(x, idx)\n",
    "\n",
    "        # node the idx is sorted\n",
    "        assert torch.all(torch.eq(out[0], torch.tensor([0, 1, 2])))\n",
    "\n",
    "        # where idx == 0\n",
    "        assert torch.all(torch.eq(out[1][0], torch.tensor([2])))\n",
    "\n",
    "        # where idx == 1\n",
    "        assert torch.all(torch.eq(out[1][1], torch.tensor([3, 4, 5])))\n",
    "\n",
    "        # where idx == 2\n",
    "        assert torch.all(torch.eq(out[1][2], torch.tensor([0, 1, 6])))\n",
    "\n",
    "    :param x: tensor to group\n",
    "    :param idx: indices\n",
    "    :return: tuple of unique, sorted indices and a list of tensors corresponding to the groups\n",
    "    \"\"\"\n",
    "    return _jit_scatter_group(x, idx, {})\n",
    "\n",
    "\n",
    "idx = torch.tensor([0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
    "x = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float)\n",
    "x.requires_grad = True\n",
    "\n",
    "scatter_group(x, idx)[1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "a = torch.tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=torch.float)\n",
    "a.requires_grad = True\n",
    "\n",
    "from pyrographnets.utils import scatter_group\n",
    "from pyrographnets.utils.scatter_group import _jit_scatter_group\n",
    "# def scatter_insert(arr: torch.Tensor, idx: torch.Tensor, insert_arrs: List[torch.Tensor]):\n",
    "#     _, groups = scatter_group(arr, arr)\n",
    "#     new_arrs = []\n",
    "#     for i, group in enumerate(groups):\n",
    "#         if i in idx:\n",
    "#             new_arrs.append(insert_arrs[i])\n",
    "#         new_arrs.append(group)\n",
    "#     return torch.cat(new_arrs)\n",
    "        \n",
    "\n",
    "# scatter_insert(a, )\n",
    "\n",
    "# this would be the graph idx\n",
    "\n",
    "idx1 = torch.tensor([0, 0, 0, 1, 1, 1, 2, 2, 3, 3])\n",
    "x1 = torch.randn(10, 5)\n",
    "idx2 = torch.tensor([0, 1, 2])\n",
    "x2 = torch.randn(3, 5)\n",
    "\n",
    "\n",
    "i1, groups1 = scatter_group(x1, idx1)\n",
    "i2, groups2 = scatter_group(x2, idx2)\n",
    "\n",
    "def dict_collate(d1, d2, collate_fn):\n",
    "    d = {}\n",
    "    for k, v in d1.items():\n",
    "        if k not in d:\n",
    "            d[k] = [v]\n",
    "        else:\n",
    "            d[k].append(v)\n",
    "    for k, v in d2.items():\n",
    "        if k not in d:\n",
    "            d[k] = [v]\n",
    "        else:\n",
    "            d[k].append(v)\n",
    "    return {k: collate_fn(v) for k, v in d.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1, 1, 2, 2, 3, 3])\n",
      "tensor([0, 1, 2])\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {k.item(): v for k, v in zip(i1, groups1)}\n",
    "d2 = {k.item(): v for k, v in zip(i2, groups2)}\n",
    "d = dict_merge(d1, d2, torch.cat)\n",
    "keys = sorted(d)\n",
    "torch.cat([d[k] for k in keys])\n",
    "\n",
    "# collect node indices\n",
    "node_idx = []\n",
    "delta_edges = []\n",
    "i = 0\n",
    "for k in keys:\n",
    "    if k in d2:\n",
    "        delta_edges += [i] * d1[k].shape[0] + [i + d2[k].shape[0]] * d2[k].shape[0]\n",
    "    else:\n",
    "        delta_edges += [i] * d1[k].shape[0]\n",
    "    i += 1\n",
    "    node_idx += [k] * d[k].shape[0]\n",
    "\n",
    "print(idx1)\n",
    "print(idx2)\n",
    "print(node_idx)\n",
    "delta_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 0.],\n",
       "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 0.]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = torch.randint(0, 10, (2, 20), dtype=torch.float)\n",
    "edges.requires_grad = True\n",
    "data = torch.randn(10)\n",
    "\n",
    "class Mask(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.blocks = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 20),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, edges, x):\n",
    "        out = self.blocks(x)\n",
    "#         return out\n",
    "        i = torch.where(torch.round(out) == 1)[0]\n",
    "        return edges[:, i]\n",
    "\n",
    "model = Mask()\n",
    "model.zero_grad()\n",
    "out = model(edges, data)\n",
    "print(out.shape)\n",
    "out.sum().backward()\n",
    "p = list(model.parameters())[-1]\n",
    "# model.blocks[0].weight.grad\n",
    "edges.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([]).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
